"ğ—›ğ—¼ğ˜„ ğ—ºğ˜‚ğ—°ğ—µ ğ—šğ—£ğ—¨ ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—¶ğ˜€ ğ—»ğ—²ğ—²ğ—±ğ—²ğ—± ğ˜ğ—¼ ğ˜€ğ—²ğ—¿ğ˜ƒğ—² ğ—® ğ—Ÿğ—®ğ—¿ğ—´ğ—² ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ (ğ—Ÿğ—Ÿğ— )?"

Whether you're deploying a 7B parameter model or something larger, estimating the GPU memory is critical.
So, what's the math behind it? Here's a formula that can help you estimate the GPU memory required for serving an LLM:

**ğ—  = ((ğ—£ * ğŸ°ğ—•) / (ğŸ¯ğŸ® / ğ—¤)) * ğŸ­.ğŸ®**

Where:

**ğŒ** is the GPU memory in Gigabytes.
**ğ** is the number of parameters in the model.
**ğŸ’ğ** represents the 4 bytes used per parameter.
**ğ** is the number of bits for loading the model.
**ğŸ.ğŸ** accounts for a 20% overhead.

**ğ—§ğ—µğ—¶ğ˜€ ğ—¼ğ˜ƒğ—²ğ—¿ğ—µğ—²ğ—®ğ—± ğ—¶ğ˜€ğ—»'ğ˜ ğ—·ğ˜‚ğ˜€ğ˜ ğ—® ğ—¯ğ˜‚ğ—³ğ—³ğ—²ğ—¿ â€”** it's crucial for additional memory used during inference, such as storing activations (intermediate results) of the model. Without accounting for this, you might underestimate the memory requirements, leading to bottlenecks during inference.

Ref:
1. [ğ—›ğ—¼ğ˜„ ğ—ºğ˜‚ğ—°ğ—µ ğ—šğ—£ğ—¨ ğ—ºğ—²ğ—ºğ—¼ğ—¿ğ˜† ğ—¶ğ˜€ ğ—»ğ—²ğ—²ğ—±ğ—²ğ—± ğ˜ğ—¼ sğ—²ğ—¿ğ˜ƒğ—² ğ—® ğ—Ÿğ—®ğ—¿ğ—´ğ—² ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹](https://ksingh7.medium.com/calculate-how-much-gpu-memory-you-need-to-serve-any-llm-67301a844f21)