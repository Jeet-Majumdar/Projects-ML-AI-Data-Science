Following from Andrej Karpathy video: [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)

Key Points:
1. Attention mechanism as a complex weighted aggregation in real time
2. Implementing attention from scratch (Basic)
3. Mathematical trick in self-attention (Using Matrix Multiplication)

Resume from: https://youtu.be/kCc8FmEb1nY?t=4757
My Colab sheet: https://colab.research.google.com/drive/113nSOmMlqAGs282qTYLcVfZSxV8vZu1z#scrollTo=CGdaOtLJ_vpq